{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.autograd import Variable \n",
    "from torch import Tensor as t\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam \n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#%matplotlib inline \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n2p(x, requires_grad = True):\n",
    "    \"\"\"converts numpy tensor to pytorch variable\"\"\"\n",
    "    return Variable(t(x), requires_grad)\n",
    "\n",
    "def t2c(x):\n",
    "    return x.cuda()\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/2591\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        dim: An integer.\n",
    "        keepdim: A boolean.\n",
    "\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if dim is None:\n",
    "        inputs = inputs.view(-1)\n",
    "        dim = 0\n",
    "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "    if not keepdim:\n",
    "        outputs = outputs.squeeze(dim)\n",
    "    return outputs\n",
    "\n",
    "def map2LieAlgebra(v):\n",
    "    \"\"\"Map a point in R^N to the tangent space at the identity, i.e. \n",
    "    to the Lie Algebra\n",
    "    Arg:\n",
    "        v = vector in R^N, (..., 3) in our case\n",
    "    Return:\n",
    "        R = v converted to Lie Algebra element, (3,3) in our case\"\"\"\n",
    "    \n",
    "    # make sure this is a sample from R^3\n",
    "    assert v.size()[-1] == 3\n",
    "    \n",
    "    R_x = n2p(np.array([[ 0., 0., 0.],\n",
    "                        [ 0., 0.,-1.],\n",
    "                        [ 0., 1., 0.]]))\n",
    "    \n",
    "    R_y = n2p(np.array([[ 0., 0., 1.],\n",
    "                        [ 0., 0., 0.],\n",
    "                        [-1., 0., 0.]]))\n",
    "    \n",
    "    R_z = n2p(np.array([[ 0.,-1., 0.],\n",
    "                        [ 1., 0., 0.],\n",
    "                        [ 0., 0., 0.]]))\n",
    "    \n",
    "    R = R_x * v[..., 0, None, None] + \\\n",
    "        R_y * v[..., 1, None, None] + \\\n",
    "        R_z * v[..., 2, None, None]\n",
    "    return R\n",
    "\n",
    "def rodrigues(v):\n",
    "    theta = v.norm(p=2,dim=-1, keepdim=True)\n",
    "    # normalize K\n",
    "    K = map2LieAlgebra(v/theta)\n",
    "    \n",
    "    I = Variable(torch.eye(3))\n",
    "    R = I + torch.sin(theta)[...,None]*K + (1. - torch.cos(theta))[...,None]*(K@K)\n",
    "    a = torch.sin(theta)[...,None]\n",
    "    return R\n",
    "\n",
    "def log_density(v, L, D, k = 10):\n",
    "    theta = v.norm(p=2,dim=-1, keepdim=True)\n",
    "    u = v / theta\n",
    "    angles = Variable(torch.arange(-k, k+1) * 2 * math.pi)\n",
    "    theta_hat = theta[...,None] + angles\n",
    "    \n",
    "#     print(theta_hat.min())\n",
    "    \n",
    "    x = u[...,None] * theta_hat\n",
    "    \n",
    "    L_hat = L - Variable(torch.eye(3))\n",
    "    L_inv = Variable(torch.eye(3)) - L_hat + L_hat @ L_hat\n",
    "    D_inv = 1. / D\n",
    "    A = L_inv @ x\n",
    "    \n",
    "#     print(A * D_inv[...,None] * A + 2 )\n",
    "#     print(theta)\n",
    "    \n",
    "    p = -0.5*(A * D_inv[...,None] * A + 2 * torch.log(theta_hat.abs()) - torch.log(2 - 2 * torch.cos(theta_hat)) ).sum(-2) \n",
    "    p = logsumexp(p, -1)\n",
    "    p += -0.5*(torch.log(D.prod(-1)) + v.size()[-1]*math.log(2.*math.pi))\n",
    "    return p\n",
    "\n",
    "def randomR():\n",
    "    q, r = np.linalg.qr(np.random.normal(size=(3, 3)))\n",
    "    r = np.diag(r)\n",
    "    ret = q @ np.diag(r / np.abs(r))\n",
    "    return ret * np.linalg.det(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(784, n_hidden)\n",
    "        self.hidden_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_mu = nn.Linear(n_hidden, 3)\n",
    "        self.hidden_Ldiag = nn.Linear(n_hidden, 3)\n",
    "        self.hidden_Lnondiag = nn.Linear(n_hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = F.tanh(self.hidden_1(x))\n",
    "        h1 = F.tanh(self.hidden_2(h0))\n",
    "        \n",
    "        mu = self.hidden_mu(h1)\n",
    "        D = F.softplus(self.hidden_Ldiag(h1))\n",
    "        L = self.hidden_Lnondiag(h1)\n",
    "\n",
    "        L = torch.cat((Variable(torch.ones(torch.Size((*D.size()[:-1], 1)))),\n",
    "                Variable(torch.zeros(torch.Size((*D.size()[:-1], 2)))),\n",
    "                L[...,0].unsqueeze(-1),\n",
    "                Variable(torch.ones(torch.Size((*D.size()[:-1], 1)))),\n",
    "                Variable(torch.zeros(torch.Size((*D.size()[:-1], 1)))),\n",
    "                L[...,1:],\n",
    "                Variable(torch.ones(torch.Size((*D.size()[:-1], 1))))), -1).view(\n",
    "            torch.Size((*D.size()[:-1], 3, 3)))\n",
    "\n",
    "        return mu, L, D\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(9, n_hidden)\n",
    "        self.hidden_2 = nn.Linear(n_hidden, 784)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = F.tanh(self.hidden_1(x))\n",
    "        h1 = F.sigmoid(self.hidden_2(h0))\n",
    "        \n",
    "        return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = Encoder(128)\n",
    "dec = Decoder(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(list(enc.parameters()) + list(dec.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing..."
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "batch_size = 32\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=  transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, (x, t_) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = Variable(x).view(-1, 784)\n",
    "    mu, L, D = enc(x)\n",
    "    \n",
    "    noise = Variable(Normal(t(np.zeros(3)), t(np.ones(3))).sample_n(batch_size))    \n",
    "    v = (L @ (D.pow(0.5)*noise)[..., None]).squeeze()\n",
    "    \n",
    "    H = -log_density(v, L, D, k = 10)\n",
    "    \n",
    "    mu_lie = rodrigues(mu)\n",
    "    v_lie = rodrigues(v)\n",
    "    g_lie = mu_lie @ v_lie\n",
    "    z_rot = g_lie\n",
    "    x_ = dec(z_rot.view(-1, 9))\n",
    "    \n",
    "    loss_bce = nn.BCELoss(size_average=False)(x_, x) / batch_size\n",
    "    loss_H = H.mean()\n",
    "    loss = loss_bce - 0 * loss_H\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('\\r', loss_bce.data.numpy()[0], loss_H.data.numpy()[0], end='')\n",
    "    \n",
    "    z_ = np.array([randomR() for _ in range(32)])\n",
    "    _plot_digits(8, 4, dec(n2p(z_).view(-1, 9)).data.numpy())\n",
    "    _plot_digits(8, 4, x_.data.numpy())\n",
    "    print(H)\n",
    "    lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _plot_digits(w, h, x):\n",
    "    h_, w_ = (28, 28)\n",
    "\n",
    "    plt.figure(figsize=(w, h))\n",
    "\n",
    "    image = np.zeros((h_ * h, w_ * w))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            image[h_ * i : h_ * (i + 1), w_ * j : w_ * (j + 1)] = x[i * w + j].reshape(28,28)\n",
    "\n",
    "    plt.imshow(image, interpolation='none', cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch3]",
   "language": "python",
   "name": "conda-env-pytorch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
