{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.autograd import Variable \n",
    "from torch import Tensor as t\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam \n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n2p(x, requires_grad = True):\n",
    "    \"\"\"converts numpy tensor to pytorch variable\"\"\"\n",
    "    return Variable(t(x), requires_grad)\n",
    "\n",
    "def t2c(x):\n",
    "    return x.cuda()\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/2591\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        dim: An integer.\n",
    "        keepdim: A boolean.\n",
    "\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if dim is None:\n",
    "        inputs = inputs.view(-1)\n",
    "        dim = 0\n",
    "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "    if not keepdim:\n",
    "        outputs = outputs.squeeze(dim)\n",
    "    return outputs\n",
    "\n",
    "def map2LieAlgebra(v):\n",
    "    \"\"\"Map a point in R^N to the tangent space at the identity, i.e. \n",
    "    to the Lie Algebra\n",
    "    Arg:\n",
    "        v = vector in R^N, (..., 3) in our case\n",
    "    Return:\n",
    "        R = v converted to Lie Algebra element, (3,3) in our case\"\"\"\n",
    "    \n",
    "    # make sure this is a sample from R^3\n",
    "    assert v.size()[-1] == 3\n",
    "    \n",
    "    R_x = n2p(np.array([[ 0., 0., 0.],\n",
    "                        [ 0., 0.,-1.],\n",
    "                        [ 0., 1., 0.]]))\n",
    "    \n",
    "    R_y = n2p(np.array([[ 0., 0., 1.],\n",
    "                        [ 0., 0., 0.],\n",
    "                        [-1., 0., 0.]]))\n",
    "    \n",
    "    R_z = n2p(np.array([[ 0.,-1., 0.],\n",
    "                        [ 1., 0., 0.],\n",
    "                        [ 0., 0., 0.]]))\n",
    "    \n",
    "    R = R_x * v[..., 0, None, None] + \\\n",
    "        R_y * v[..., 1, None, None] + \\\n",
    "        R_z * v[..., 2, None, None]\n",
    "    return R\n",
    "\n",
    "def rodrigues(v):\n",
    "    theta = v.norm(p=2,dim=-1, keepdim=True)\n",
    "    # normalize K\n",
    "    K = map2LieAlgebra(v/theta)\n",
    "    \n",
    "    I = Variable(torch.eye(3))\n",
    "    R = I + torch.sin(theta)[...,None]*K + (1. - torch.cos(theta))[...,None]*(K@K)\n",
    "    a = torch.sin(theta)[...,None]\n",
    "    return R\n",
    "\n",
    "def log_density(v, L, D, k = 10):\n",
    "    theta = v.norm(p=2,dim=-1, keepdim=True)\n",
    "    u = v / theta\n",
    "    angles = Variable(torch.arange(-k, k+1) * 2 * math.pi)\n",
    "    theta_hat = theta[...,None] + angles\n",
    "    \n",
    "#     print(theta_hat.min())\n",
    "    \n",
    "    x = u[...,None] * theta_hat\n",
    "    \n",
    "    L_hat = L - Variable(torch.eye(3))\n",
    "    L_inv = Variable(torch.eye(3)) - L_hat + L_hat @ L_hat\n",
    "    D_inv = 1. / D\n",
    "    A = L_inv @ x\n",
    "    \n",
    "#     print(A * D_inv[...,None] * A + 2 )\n",
    "#     print(theta)\n",
    "    \n",
    "    p = -0.5*(A * D_inv[...,None] * A + 2 * torch.log(theta_hat.abs()) - torch.log(2 - 2 * torch.cos(theta_hat)) ).sum(-2) \n",
    "    p = logsumexp(p, -1)\n",
    "    p += -0.5*(torch.log(D.prod(-1)) + v.size()[-1]*math.log(2.*math.pi))*(2*k + 1)\n",
    "    return p\n",
    "\n",
    "def randomR():\n",
    "    q, r = np.linalg.qr(np.random.normal(size=(3, 3)))\n",
    "    r = np.diag(r)\n",
    "    ret = q @ np.diag(r / np.abs(r))\n",
    "    return ret * np.linalg.det(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(784, n_hidden)\n",
    "        self.hidden_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_mu = nn.Linear(n_hidden, 3)\n",
    "        self.hidden_Ldiag = nn.Linear(n_hidden, 3)\n",
    "        self.hidden_Lnondiag = nn.Linear(n_hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = F.tanh(self.hidden_1(x))\n",
    "        h1 = F.tanh(self.hidden_2(h0))\n",
    "        \n",
    "        mu = self.hidden_mu(h1)\n",
    "        D = F.softplus(self.hidden_Ldiag(h1))\n",
    "        L = self.hidden_Lnondiag(h1)\n",
    "\n",
    "        L = torch.cat((Variable(torch.ones(torch.Size((*D.size()[:-1], 1)))),\n",
    "                Variable(torch.zeros(torch.Size((*D.size()[:-1], 2)))),\n",
    "                L[...,0].unsqueeze(-1),\n",
    "                Variable(torch.ones(torch.Size((*D.size()[:-1], 1)))),\n",
    "                Variable(torch.zeros(torch.Size((*D.size()[:-1], 1)))),\n",
    "                L[...,1:],\n",
    "                Variable(torch.ones(torch.Size((*D.size()[:-1], 1))))), -1).view(\n",
    "            torch.Size((*D.size()[:-1], 3, 3)))\n",
    "\n",
    "        return mu, L, D\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(9, n_hidden)\n",
    "        self.hidden_2 = nn.Linear(n_hidden, 784)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = F.tanh(self.hidden_1(x))\n",
    "        h1 = F.sigmoid(self.hidden_2(h0))\n",
    "        \n",
    "        return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(128)\n",
    "dec = Decoder(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(list(enc.parameters()) + list(dec.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "batch_size = 32\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=  transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 154.48254 nan4.220587"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -nan at /opt/conda/conda-bld/pytorch-cpu_1515613813020/work/torch/lib/THNN/generic/BCECriterion.c:34",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-6c496903bc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_rot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss_bce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mloss_H\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_bce\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 372\u001b[0;31m                                       size_average=self.size_average)\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -nan at /opt/conda/conda-bld/pytorch-cpu_1515613813020/work/torch/lib/THNN/generic/BCECriterion.c:34"
     ]
    }
   ],
   "source": [
    "for idx, (x, t_) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = Variable(x).view(-1, 784)\n",
    "    mu, L, D = enc(x)\n",
    "    \n",
    "    noise = Variable(Normal(t(np.zeros(3)), t(np.ones(3))).sample_n(batch_size))    \n",
    "    v = (L @ (D.pow(0.5)*noise)[..., None]).squeeze()\n",
    "    \n",
    "    H = -log_density(v, L, D, k = 10)\n",
    "    \n",
    "    mu_lie = rodrigues(mu)\n",
    "    v_lie = rodrigues(v)\n",
    "    g_lie = mu_lie @ v_lie\n",
    "    z_rot = g_lie\n",
    "    x_ = dec(z_rot.view(-1, 9))\n",
    "    \n",
    "    loss_bce = nn.BCELoss(size_average=False)(x_, x) / batch_size\n",
    "    loss_H = H.mean()\n",
    "    loss = loss_bce - 0 * loss_H\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('\\r', loss_bce.data.numpy()[0], loss_H.data.numpy()[0], end='')\n",
    "    \n",
    "    z_ = np.array([randomR() for _ in range(32)])\n",
    "    _plot_digits(8, 4, dec(n2p(z_).view(-1, 9)).data.numpy())\n",
    "    _plot_digits(8, 4, x_.data.numpy())\n",
    "    print(H)\n",
    "    lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_digits(w, h, x):\n",
    "    h_, w_ = (28, 28)\n",
    "\n",
    "    plt.figure(figsize=(w, h))\n",
    "\n",
    "    image = np.zeros((h_ * h, w_ * w))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            image[h_ * i : h_ * (i + 1), w_ * j : w_ * (j + 1)] = x[i * w + j].reshape(28,28)\n",
    "\n",
    "    plt.imshow(image, interpolation='none', cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
