{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.autograd import Variable \n",
    "from torch import Tensor as t\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n2p(x, requires_grad = True):\n",
    "    \"\"\"converts numpy tensor to pytorch variable\"\"\"\n",
    "    return Variable(t(x), requires_grad)\n",
    "\n",
    "def t2c(x):\n",
    "    return x.cuda()\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/2591\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        dim: An integer.\n",
    "        keepdim: A boolean.\n",
    "\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if dim is None:\n",
    "        inputs = inputs.view(-1)\n",
    "        dim = 0\n",
    "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "    if not keepdim:\n",
    "        outputs = outputs.squeeze(dim)\n",
    "    return outputs\n",
    "\n",
    "def deg2coo(x):\n",
    "    theta = x.T[0]\n",
    "    phi = x.T[1]\n",
    "\n",
    "    xs = np.sin(phi) * np.sin(theta)\n",
    "    ys = np.cos(phi) * np.sin(theta)\n",
    "    zs = np.cos(theta)\n",
    "\n",
    "    return np.vstack((xs, ys, zs)).T\n",
    "\n",
    "def randomR():\n",
    "    q, r = np.linalg.qr(np.random.normal(size=(3, 3)))\n",
    "    r = np.diag(r)\n",
    "    ret = q @ np.diag(r / np.abs(r))\n",
    "    return ret * np.linalg.det(ret)\n",
    "\n",
    "def next_batch(batch_dim):\n",
    "    a = np.pi / 12\n",
    "    canonicalL = deg2coo(np.array([[0, 0],\n",
    "                                   [a / 2, np.pi/2],\n",
    "                                   [a, np.pi/2],\n",
    "                                   [a / 2, 0],\n",
    "                                   [a, 0]]))\n",
    "\n",
    "    originalL = np.stack([canonicalL @ randomR() for _ in range(batch_dim)])\n",
    "    rotations = np.stack([randomR() for _ in range(batch_dim)])\n",
    "    rotatedL = np.stack([oL @ rot for oL, rot in zip(originalL, rotations)])\n",
    "\n",
    "    return originalL, rotatedL, rotations\n",
    "\n",
    "def get_sample(N, x_mb):\n",
    "    mu, L, D = N(n2p(x_mb))\n",
    "    noise = Variable(Normal(t(np.zeros(3)), t(np.ones(3))).sample_n(1))    \n",
    "    v = (L @ (D.pow(0.5)*noise)[..., None]).squeeze()\n",
    "    mu_lie = rodrigues(mu)\n",
    "    v_lie = rodrigues(v)\n",
    "    g_lie = mu_lie @ v_lie\n",
    "\n",
    "    xrot_recon = (n2p(xo) @ g_lie).data.numpy()\n",
    "    return xrot_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP outputs $L$, $\\epsilon$ and $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(2 * 5 * 3, n_hidden)\n",
    "        self.hidden_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_mu = nn.Linear(n_hidden, 3)\n",
    "        self.hidden_Ldiag = nn.Linear(n_hidden, 3)\n",
    "        self.hidden_Lnondiag = nn.Linear(n_hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = F.tanh(self.hidden_1(x))\n",
    "        h1 = F.tanh(self.hidden_2(h0))\n",
    "        \n",
    "        mu = self.hidden_mu(h1)\n",
    "        D = F.softplus(self.hidden_Ldiag(h1))\n",
    "        L = self.hidden_Lnondiag(h1)\n",
    "\n",
    "        L = torch.cat((Variable(torch.ones(torch.Size((*D.size()[:-1], 1)))),\n",
    "                Variable(torch.zeros(torch.Size((*D.size()[:-1], 2)))),\n",
    "                L[...,0].unsqueeze(-1),\n",
    "                Variable(torch.ones(torch.Size((*D.size()[:-1], 1)))),\n",
    "                Variable(torch.zeros(torch.Size((*D.size()[:-1], 1)))),\n",
    "                L[...,1:],\n",
    "                Variable(torch.ones(torch.Size((*D.size()[:-1], 1))))), -1).view(\n",
    "            torch.Size((*D.size()[:-1], 3, 3)))\n",
    "\n",
    "        return mu, L, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert outputs in $\\mathbb{R}^3$ to Lie algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map2LieAlgebra(v):\n",
    "    \"\"\"Map a point in R^N to the tangent space at the identity, i.e. \n",
    "    to the Lie Algebra\n",
    "    Arg:\n",
    "        v = vector in R^N, (..., 3) in our case\n",
    "    Return:\n",
    "        R = v converted to Lie Algebra element, (3,3) in our case\"\"\"\n",
    "    \n",
    "    # make sure this is a sample from R^3\n",
    "    assert v.size()[-1] == 3\n",
    "    \n",
    "    R_x = n2p(np.array([[ 0., 0., 0.],\n",
    "                        [ 0., 0.,-1.],\n",
    "                        [ 0., 1., 0.]]))\n",
    "    \n",
    "    R_y = n2p(np.array([[ 0., 0., 1.],\n",
    "                        [ 0., 0., 0.],\n",
    "                        [-1., 0., 0.]]))\n",
    "    \n",
    "    R_z = n2p(np.array([[ 0.,-1., 0.],\n",
    "                        [ 1., 0., 0.],\n",
    "                        [ 0., 0., 0.]]))\n",
    "    \n",
    "    R = R_x * v[..., 0, None, None] + \\\n",
    "        R_y * v[..., 1, None, None] + \\\n",
    "        R_z * v[..., 2, None, None]\n",
    "    return R\n",
    "\n",
    "# x = Normal(t([0., 0., 0.]), t([1., 1., 1.])).sample_n(3)\n",
    "# v = Variable(x)\n",
    "# l = map2LieAlgebra(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a exponential map, $exp(\\cdot)$, to convert elements of Lie algebra to Lie group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rodrigues(v):\n",
    "    theta = v.norm(p=2,dim=-1, keepdim=True)\n",
    "    # normalize K\n",
    "    K = map2LieAlgebra(v/theta)\n",
    "    \n",
    "    I = Variable(torch.eye(3))\n",
    "    R = I + torch.sin(theta)[...,None]*K + (1. - torch.cos(theta))[...,None]*(K@K)\n",
    "    a = torch.sin(theta)[...,None]\n",
    "    return R\n",
    "\n",
    "# x = Normal(t([0., 0., 0.]), t([1., 1., 1.])).sample_n(2)\n",
    "# v = Variable(x)\n",
    "# R = rodrigues(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Density, not using the Jacobian change of coordinates\n",
    "$$\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(u (2\\pi k + \\theta) \\;|\\;0,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_density(v, L, D, k = 10):\n",
    "    theta = v.norm(p=2,dim=-1, keepdim=True)\n",
    "    u = v / theta\n",
    "    angles = Variable(torch.arange(-k, k+1) * 2 * math.pi)\n",
    "    theta_hat = theta[...,None] + angles\n",
    "    x = u[...,None] * theta_hat\n",
    "    \n",
    "    L_hat = L - Variable(torch.eye(3))\n",
    "    L_inv = Variable(torch.eye(3)) - L_hat + L_hat@L_hat\n",
    "    D_inv = 1. / D\n",
    "    A = L_inv @ x\n",
    "    \n",
    "    p = -0.5*(A * D_inv[...,None] * A).sum(-2)\n",
    "    p = logsumexp(p, -1)\n",
    "    p += -0.5*(torch.log(D.prod(-1)) + v.size()[-1]*math.log(2.*math.pi))*(2*k + 1)\n",
    "    return p\n",
    "    \n",
    "# x = Normal(t([0., 0., 0.]), t([1., 1., 1.])).sample_n(2)\n",
    "# v = Variable(x)\n",
    "# L = Variable((torch.rand(3,3).tril(-1) + torch.eye(3)).repeat(2, 1, 1))\n",
    "# D = Variable(torch.rand(2, 3))\n",
    "# log_density(v, L, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = Net(128)\n",
    "optimizer = Adam(N.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 5000\n",
    "loss_plot = []\n",
    "for i in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    xo_mb, xrot_mb, y_mb = next_batch(batch_size)\n",
    "    x_mb = np.hstack((xo_mb.reshape(-1, 5 * 3), xrot_mb.reshape(-1, 5 * 3)))\n",
    "    \n",
    "    mu, L, D = N(n2p(x_mb))\n",
    "    noise = Variable(Normal(t(np.zeros(3)), t(np.ones(3))).sample_n(batch_size))    \n",
    "    v = (L @ (D.pow(0.5)*noise)[..., None]).squeeze()\n",
    "    \n",
    "    H = -log_density(v, L, D, k = 10)\n",
    "    \n",
    "    mu_lie = rodrigues(mu)\n",
    "    v_lie = rodrigues(v)\n",
    "    g_lie = mu_lie @ v_lie\n",
    "    z_rot = g_lie\n",
    "    \n",
    "    xrot_recon_mb = n2p(xo_mb) @ z_rot\n",
    "    # Spherical Loss\n",
    "    L_rec = ((xrot_recon_mb * n2p(xrot_mb)).sum(-1) * 0.999).acos().sum(-1).mean()\n",
    "    # L2 loss\n",
    "#     L_rec = (( xrot_recon_mb- n2p(xrot_mb))**2).sum(-1).sum(-1).mean()\n",
    "    # weighting the entropy term to not be too strong\n",
    "    kl_w = 0.#1e-2\n",
    "    L = L_rec - torch.mean(H)*kl_w\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    print ('\\r (%d/%d) L: %.3f \\t D: %.3f ' % \n",
    "           (i, num_steps, L_rec.data.numpy(), np.mean(D.data.numpy())), end='')\n",
    "    loss_plot.append(L_rec.data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot, color='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hammer(data):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(111, projection='hammer')\n",
    "\n",
    "    cols = ('blue', 'red', 'green', 'cyan', 'magenta', 'pink', 'black', 'purple', 'brown', 'orange')\n",
    "    for i in range(data.shape[0]):\n",
    "        xs_, ys_, zs_ = data[i,:].reshape(-1, 3).T\n",
    "        ys = np.arccos(zs_) - np.pi/2\n",
    "        xs = np.arctan2(ys_, xs_)\n",
    "\n",
    "        plt.scatter(xs, ys, color=cols[i], label=str(i))\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.xticks_labels([], [])\n",
    "    # plt.yticks([], [])\n",
    "    # plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_sphere():\n",
    "    #plot sphere\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "    x = 1 * np.outer(np.cos(u), np.sin(v))\n",
    "    y = 1 * np.outer(np.sin(u), np.sin(v))\n",
    "    z = 1 * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "    ax.plot_surface(x, y, z,  rstride=4, cstride=4, color='#DAE8FC', linewidth=0, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# get an input and rotation\n",
    "xo, xrot, y = next_batch(1)\n",
    "x_mb = np.hstack((xo.reshape(-1, 5 * 3), xrot.reshape(-1, 5 * 3)))\n",
    "# plot the original and rotation\n",
    "xs, ys, zs = xo.reshape(-1, 3).T\n",
    "ax.scatter(xs, ys, zs, c='b')\n",
    "xs, ys, zs = xrot.reshape(-1, 3).T\n",
    "ax.scatter(xs, ys, zs, c='r')\n",
    "\n",
    "# get samples from neural net\n",
    "n_samples = 1\n",
    "samples = xrot\n",
    "for _ in range(n_samples):\n",
    "    xrot_recon = get_sample(N, x_mb)\n",
    "    xs, ys, zs = xrot_recon.reshape(-1, 3).T\n",
    "    ax.scatter(xs, ys, zs)\n",
    "    \n",
    "    samples = np.hstack((samples, xrot_recon))\n",
    "\n",
    "plot_sphere()\n",
    "    \n",
    "ax.grid(False)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "data = np.hstack((xo, samples)).reshape(2 + n_samples,5,3)\n",
    "plot_hammer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
