{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from s2cnn.nn.soft.so3_conv import SO3Convolution\n",
    "from s2cnn.nn.soft.s2_conv import S2Convolution\n",
    "from s2cnn.nn.soft.so3_integrate import so3_integrate\n",
    "from s2cnn.ops.so3_localft import near_identity_grid as so3_near_identity_grid\n",
    "from s2cnn.ops.s2_localft import near_identity_grid as s2_near_identity_grid\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_data(path, batch_size):\n",
    "\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    train_data = torch.from_numpy(\n",
    "        dataset[\"train\"][\"images\"][:,None,:,:].astype(np.float32))\n",
    "    train_labels = torch.from_numpy(\n",
    "        dataset[\"train\"][\"labels\"].astype(np.int64))\n",
    "\n",
    "    mean = train_data.mean()\n",
    "    stdv = train_data.std()\n",
    "\n",
    "    train_dataset = data_utils.TensorDataset(train_data, train_labels)\n",
    "    train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data = torch.from_numpy(\n",
    "        dataset[\"test\"][\"images\"][:,None,:,:].astype(np.float32))\n",
    "    test_labels = torch.from_numpy(\n",
    "        dataset[\"test\"][\"labels\"].astype(np.int64))\n",
    "\n",
    "    test_dataset = data_utils.TensorDataset(test_data, test_labels)\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader, train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self ,H = [1,10,1], activation = nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "        self.H = H\n",
    "        self.activation = activation()\n",
    "        modules = []\n",
    "        for input_dim, output_dim in zip (H, H[1:-1]):\n",
    "            modules.append(nn.Linear(input_dim, output_dim))\n",
    "            modules.append(self.activation)\n",
    "        modules.append(nn.Linear(H[-2],H[-1]))\n",
    "        self.module = nn.Sequential(*modules)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.module(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class S2ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, f_list = [1,10,10], b_list = [30,10,6], mlp_dim = [10], activation = nn.ReLU):\n",
    "        super(S2ConvNet, self).__init__()\n",
    "        \n",
    "        #TODO make boolean for integrate\n",
    "        grid_s2 = s2_near_identity_grid()\n",
    "        grid_so3 = so3_near_identity_grid()\n",
    "        \n",
    "        self.f_list = f_list\n",
    "        self.b_list = b_list\n",
    "        \n",
    "        #self.mlp_dim = mlp_dim.copy()\n",
    "        \n",
    "        #self.mlp_dim.insert(0, f_list[-1]*(b_list[-1]*2)**3)\n",
    "        #print(self.mlp_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "        modules = []\n",
    "        conv1 = S2Convolution(\n",
    "            nfeature_in= f_list[0],\n",
    "            nfeature_out=f_list[1],\n",
    "            b_in=b_list[0],\n",
    "            b_out=b_list[1],\n",
    "            grid=grid_s2)\n",
    "        modules.append(conv1)\n",
    "        modules.append(self.activation())\n",
    "    \n",
    "        for f_in, f_out, b_in, b_out in zip(f_list[1:-1], f_list[2:], b_list[1:-1], b_list[2:]):\n",
    "            #print(f_in, f_out, b_in, b_out)\n",
    "            conv = SO3Convolution(nfeature_in=f_in,\n",
    "                                  nfeature_out=f_out,\n",
    "                                  b_in=b_in,\n",
    "                                  b_out=b_out,\n",
    "                                  grid=grid_so3)\n",
    "            \n",
    "            modules.append(conv)\n",
    "            modules.append(self.activation())\n",
    "        self.conv_module = nn.Sequential(*modules) \n",
    "        self.mlp_module = MLP(H = self.mlp_dim, activation = self.activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_module(x)\n",
    "        x = self.mlp_module(x.view(-1,self.mlp_dim[0]))\n",
    "        #x = so3_integrate(x)\n",
    "\n",
    "        return x\n",
    "a = S2ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class S2DeconvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, f_list = [10,10,1], b_list = [5,10,30], mlp_dim = [10], activation = nn.ReLU):\n",
    "        super(S2DeconvNet, self).__init__()\n",
    "        #TODO make boolean for integrate\n",
    "        grid_s2 = s2_near_identity_grid()\n",
    "        grid_so3 = so3_near_identity_grid()\n",
    "        \n",
    "        self.f_list = f_list\n",
    "        self.b_list = b_list\n",
    "        self.mlp_dim = mlp_dim.copy()\n",
    "        self.mlp_dim.append( f_list[0]*(b_list[0]*2)**3)\n",
    "        print(self.mlp_dim)\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.mlp_module = MLP(H = self.mlp_dim, activation = self.activation)\n",
    "\n",
    "        modules = []\n",
    "        for f_in, f_out, b_in, b_out in zip(f_list[:-1], f_list[1:], b_list[:-1], b_list[1:]):\n",
    "            modules.append(self.activation())\n",
    "            if b_in < b_out:\n",
    "                modules.append(torch.nn.Upsample(size=b_out*2,  mode='nearest'))\n",
    "                \n",
    "            conv = SO3Convolution(nfeature_in=f_in,\n",
    "                                  nfeature_out=f_out,\n",
    "                                  b_in=b_out,\n",
    "                                  b_out=b_out,\n",
    "                                  grid=grid_so3)\n",
    "            modules.append(conv)\n",
    "        self.conv_module = nn.Sequential(*modules) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mlp_module(x)\n",
    "        shape = x.size()[:-1]\n",
    "        x = x.view(*shape, self.f_list[0], self.b_list[0]*2, self.b_list[0]*2, self.b_list[0]*2)\n",
    "        x = self.conv_module(x)\n",
    "        #x = so3_integrate(x)\n",
    "        return x\n",
    "    \n",
    "#d = S2DeconvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_ID = 0\n",
    "MNIST_PATH = \"./mnist_example/s2_mnist.gz\"\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "train_loader, test_loader, train_dataset, _ = load_data(\n",
    "        MNIST_PATH, BATCH_SIZE)\n",
    "torch.cuda.set_device(DEVICE_ID)\n",
    "classifier = S2ConvNet()\n",
    "\n",
    "print(\"#params\", sum([x.numel() for x in classifier.parameters()]))\n",
    "if torch.cuda.is_available():\n",
    "    classifier.cuda(DEVICE_ID)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    criterion = criterion.cuda(DEVICE_ID)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    classifier.parameters(),\n",
    "    lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        print(images.max())\n",
    "\n",
    "        print(images.size())\n",
    "        print(labels.size())\n",
    "        lol()\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda(DEVICE_ID)\n",
    "            labels = labels.cuda(DEVICE_ID)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(images)\n",
    "        outputs = so3_integrate(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "           epoch+1, NUM_EPOCHS, i+1, len(train_dataset)//BATCH_SIZE,\n",
    "           loss.data[0]), end=\"\")\n",
    "    print(\"\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = Variable(images, volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda(DEVICE_ID)\n",
    "            labels = labels.cuda(DEVICE_ID)\n",
    "\n",
    "        outputs = classifier(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Test Accuracy: {0}'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = S2DeconvNet()\n",
    "d = d.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(Normal(torch.Tensor(np.zeros((10))), torch.Tensor(np.ones((10)))).sample_n(6)).cuda()\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = None, None\n",
    "for h, (k, l) in enumerate(train_loader):\n",
    "    i = k\n",
    "    j = l\n",
    "    if h > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lie_learn.spaces import S2\n",
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "img = i.numpy()[np.random.randint(0,32),...]\n",
    "img.shape\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from scipy.special import sph_harm\n",
    "%matplotlib notebook\n",
    "\n",
    "phi = np.linspace(0, np.pi, 60)\n",
    "theta = np.linspace(0, 2*np.pi, 60)\n",
    "phi, theta = np.meshgrid(phi, theta)\n",
    "\n",
    "# The Cartesian coordinates of the unit sphere\n",
    "x = np.sin(phi) * np.cos(theta)\n",
    "y = np.sin(phi) * np.sin(theta)\n",
    "z = np.cos(phi)\n",
    "\n",
    "# Set the aspect ratio to 1 so our sphere looks spherical\n",
    "fig = plt.figure(figsize=plt.figaspect(1.))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, z,  rstride=1, cstride=1, facecolors=cm.binary(img[0]))\n",
    "# Turn off the axis planes\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
