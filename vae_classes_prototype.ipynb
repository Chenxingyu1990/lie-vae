{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from s2cnn.nn.soft.so3_conv import SO3Convolution\n",
    "from s2cnn.nn.soft.s2_conv import S2Convolution\n",
    "from s2cnn.nn.soft.so3_integrate import so3_integrate\n",
    "from s2cnn.ops.so3_localft import near_identity_grid as so3_near_identity_grid\n",
    "from s2cnn.ops.s2_localft import near_identity_grid as s2_near_identity_grid\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self ,H = [1,10,1], activation = nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "        self.H = H\n",
    "        self.activation = activation()\n",
    "        modules = []\n",
    "        for input_dim, output_dim in zip (H, H[1:-1]):\n",
    "            modules.append(nn.Linear(input_dim, output_dim))\n",
    "            modules.append(self.activation)\n",
    "        modules.append(nn.Linear(H[-2],H[-1]))\n",
    "        self.module = nn.Sequential(*modules)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.module(x)\n",
    "        return y\n",
    "    \n",
    "class Nreparametrize(nn.Module):\n",
    "    def __init__(self,input_dim, z_dim):\n",
    "        super(Nreparametrize, self).__init__()\n",
    "            \n",
    "        self.input_dim = input_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.sigma_linear = nn.Linear(input_dim, z_dim)\n",
    "        self.mu_linear = nn.Linear(input_dim, z_dim)\n",
    "        \n",
    "    def forward(self, x, n = 1):\n",
    "        x = F.relu(x)\n",
    "        self.mu = self.mu_linear(x)\n",
    "        self.log_sigma = self.sigma_linear(x)  \n",
    "        self.z = self.nsample(self.mu, self.log_sigma, n = n)\n",
    "        \n",
    "        return self.z\n",
    "    \n",
    "    \n",
    "    def kl(self):\n",
    "        return -0.5 * torch.sum(1 + 2*self.log_sigma - self.mu.pow(2) - self.log_sigma.exp()**2, -1)\n",
    "        \n",
    "        \n",
    "    def log_posterior(self):\n",
    "        raise Normal(self.mu, self.log_sigma.exp()).log_prob(self.z)\n",
    "        \n",
    "    def log_prior(self):\n",
    "        return Normal(torch.ones_like(self.mu), torch.zeros_like(self.log_sigma)).log_prob(self.z)\n",
    "        \n",
    "        \n",
    "    @staticmethod     \n",
    "    def nsample(mu, log_sigma, n = 1):\n",
    "        eps = Normal(torch.ones_like(mu), torch.zeros_like(mu)).sample_n(n)\n",
    "        return mu + eps*log_sigma.exp()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = None \n",
    "        self.decoder = None \n",
    "        self.reparametrize = [None]\n",
    "        \n",
    "        \n",
    "    def encode(self,x, n = 1):\n",
    "        h = self.encoder(x)\n",
    "        self.z = [r(h, n) for r in self.reparametrize]\n",
    "        return self.z\n",
    "    \n",
    "    def kl(self):\n",
    "        # NOTE always call after encode\n",
    "        # TODO make this bahaviour impossible\n",
    "        kl = [r.kl() for r in self.reparametrize]\n",
    "        return kl\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.encode(x);\n",
    "    \n",
    "        # flatten and stack z\n",
    "        d0, d1 = self.z[0].size()[:2]\n",
    "        z_cat = torch.cat([v.view(d0,d1,-1) for v in self.z], -1)\n",
    "        \n",
    "        return self.decode(z_cat)\n",
    "    \n",
    "    @staticmethod\n",
    "    def recon_loss(x, x_recon):\n",
    "        raise NotImplemented \n",
    "    \n",
    "    def elbo(self,x):\n",
    "        x_recon = self.forward(x)\n",
    "        kl_summed  = torch.sum(torch.stack(kl,-1), -1)\n",
    "        recon_loss = self.recon_loss(x, x_recon)\n",
    "        return recon_loss, kl_summed\n",
    "        \n",
    "        \n",
    "    def log_likelihood(self, x):\n",
    "        raise NotImplemented       \n",
    "\n",
    "class NVAE(VAE):\n",
    "    def __init__(self, encoder_dims = [10, 7, 5], decoder_dims = [5,7,10]):\n",
    "        super(NVAE, self).__init__() \n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.decoder_dims = decoder_dims\n",
    "        assert encoder_dims[-1] == decoder_dims[0]\n",
    "        self.encoder = MLP(self.encoder_dims[:-1])\n",
    "        self.decoder = MLP(self.decoder_dims)\n",
    "        self.reparametrize = [Nreparametrize(self.encoder_dims[-2], self.encoder_dims[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae = NVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(Normal(torch.Tensor(np.zeros((10))), torch.Tensor(np.ones((10)))).sample_n(6))\n",
    "rec = vae(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae.kl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Upsample(size=None, scale_factor=None, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
